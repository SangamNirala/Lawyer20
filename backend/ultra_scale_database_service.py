"""
Ultra-Scale Database Service - Step 3.1 Implementation
Distributed Database Architecture for 370M+ Legal Documents
Designed for massive scale with geographic sharding and AI optimization
"""

import asyncio
import logging
import hashlib
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict
import json
import time
import statistics

from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase, AsyncIOMotorCollection
from pymongo import IndexModel, ASCENDING, DESCENDING, TEXT
from pymongo.errors import DuplicateKeyError, OperationFailure
import pymongo

from legal_models import (
    LegalDocument, LegalDocumentCreate, LegalDocumentUpdate, LegalDocumentFilter,
    LegalDocumentResponse, DocumentType, JurisdictionLevel, ProcessingStatus,
    LegalSource, LegalScrapingJob, SystemMetrics, SourceAnalytics
)

logger = logging.getLogger(__name__)

@dataclass 
class ShardConfiguration:
    """Configuration for database shards"""
    shard_name: str
    primary_jurisdictions: List[str]
    document_types: List[DocumentType]
    estimated_capacity: int
    priority_level: int
    geographic_region: str
    
    # Performance settings
    read_preference: str = "primary"
    write_concern: int = 1
    max_pool_size: int = 100
    
    # Sharding strategy
    shard_key: str = "jurisdiction"
    chunk_size: int = 64  # MB

@dataclass
class QueryPerformanceMetrics:
    """Track query performance across shards"""
    query_type: str
    shard_name: str
    execution_time_ms: float
    documents_scanned: int
    documents_returned: int
    index_used: bool
    timestamp: datetime = field(default_factory=datetime.utcnow)

class GeographicShardingStrategy:
    """AI-powered geographic sharding strategy for optimal performance"""
    
    def __init__(self):
        # Step 3.1: Define sharding strategy for 370M+ documents
        self.shard_configurations = {
            'us_federal': ShardConfiguration(
                shard_name='us_federal',
                primary_jurisdictions=['United States Federal', 'US Federal', 'United States'],
                document_types=[DocumentType.CASE_LAW, DocumentType.STATUTE, DocumentType.REGULATION],
                estimated_capacity=100_000_000,  # 100M documents
                priority_level=1,
                geographic_region='North America',
                max_pool_size=150
            ),
            
            'us_state': ShardConfiguration(
                shard_name='us_state', 
                primary_jurisdictions=[
                    'California', 'New York', 'Texas', 'Florida', 'Illinois',
                    'Pennsylvania', 'Ohio', 'Georgia', 'North Carolina', 'Michigan'
                    # All 50 US states
                ],
                document_types=[DocumentType.CASE_LAW, DocumentType.STATUTE, DocumentType.REGULATION],
                estimated_capacity=50_000_000,  # 50M documents
                priority_level=1,
                geographic_region='North America',
                max_pool_size=120
            ),
            
            'european_union': ShardConfiguration(
                shard_name='european_union',
                primary_jurisdictions=[
                    'European Union', 'Germany', 'France', 'Italy', 'Spain',
                    'Netherlands', 'Belgium', 'Austria', 'Sweden', 'Denmark'
                    # All 27 EU member states
                ],
                document_types=[DocumentType.CASE_LAW, DocumentType.REGULATION, DocumentType.TREATY],
                estimated_capacity=25_000_000,  # 25M documents
                priority_level=1,
                geographic_region='Europe',
                max_pool_size=100
            ),
            
            'commonwealth': ShardConfiguration(
                shard_name='commonwealth',
                primary_jurisdictions=[
                    'United Kingdom', 'Canada', 'Australia', 'New Zealand', 
                    'India', 'South Africa', 'Singapore', 'Hong Kong'
                ],
                document_types=[DocumentType.CASE_LAW, DocumentType.STATUTE, DocumentType.ADMINISTRATIVE],
                estimated_capacity=30_000_000,  # 30M documents
                priority_level=1,
                geographic_region='Commonwealth',
                max_pool_size=100
            ),
            
            'asia_pacific': ShardConfiguration(
                shard_name='asia_pacific',
                primary_jurisdictions=[
                    'Japan', 'South Korea', 'China', 'Taiwan', 'Thailand',
                    'Malaysia', 'Philippines', 'Indonesia', 'Vietnam'
                ],
                document_types=[DocumentType.CASE_LAW, DocumentType.STATUTE, DocumentType.ADMINISTRATIVE],
                estimated_capacity=50_000_000,  # 50M documents
                priority_level=2,
                geographic_region='Asia Pacific',
                max_pool_size=80
            ),
            
            'academic': ShardConfiguration(
                shard_name='academic',
                primary_jurisdictions=['International', 'Academic'],
                document_types=[DocumentType.SCHOLARLY_ARTICLE, DocumentType.LEGAL_BRIEF],
                estimated_capacity=50_000_000,  # 50M documents
                priority_level=1,
                geographic_region='Global',
                max_pool_size=100,
                shard_key='document_type'
            ),
            
            'professional': ShardConfiguration(
                shard_name='professional',
                primary_jurisdictions=['Professional Organizations', 'Bar Associations'],
                document_types=[DocumentType.BAR_PUBLICATION, DocumentType.LEGAL_NEWS],
                estimated_capacity=35_000_000,  # 35M documents
                priority_level=2,
                geographic_region='Global',
                max_pool_size=80,
                shard_key='source'
            ),
            
            'specialized': ShardConfiguration(
                shard_name='specialized',
                primary_jurisdictions=['Specialized Legal Areas'],
                document_types=[
                    DocumentType.INTERNATIONAL_LAW, DocumentType.TREATY, 
                    DocumentType.EXECUTIVE_ORDER, DocumentType.AGENCY_GUIDANCE
                ],
                estimated_capacity=30_000_000,  # 30M documents
                priority_level=3,
                geographic_region='Global',
                max_pool_size=60,
                shard_key='legal_topics'
            )
        }
        
        logger.info(f"üóÑÔ∏è Initialized geographic sharding strategy with {len(self.shard_configurations)} shards")
    
    def determine_shard(self, document: Union[LegalDocument, LegalDocumentCreate]) -> str:
        """Determine optimal shard for document based on jurisdiction and type"""
        jurisdiction = document.jurisdiction.strip() if document.jurisdiction else 'Unknown'
        document_type = document.document_type
        
        # Check each shard configuration for best match
        best_shard = 'specialized'  # Default fallback
        best_score = 0
        
        for shard_name, config in self.shard_configurations.items():
            score = 0
            
            # Jurisdiction matching (70% weight)
            for primary_jurisdiction in config.primary_jurisdictions:
                if primary_jurisdiction.lower() in jurisdiction.lower():
                    score += 70
                    break
            
            # Document type matching (30% weight) 
            if document_type in config.document_types:
                score += 30
            
            # Prefer higher priority shards for ties
            if score > best_score or (score == best_score and config.priority_level < self.shard_configurations[best_shard].priority_level):
                best_score = score
                best_shard = shard_name
        
        logger.debug(f"üìç Document routed to shard '{best_shard}' (score: {best_score})")
        return best_shard
    
    def get_query_shards(self, query_filter: LegalDocumentFilter) -> List[str]:
        """Determine which shards to query based on filter criteria"""
        target_shards = set()
        
        # If specific jurisdictions requested, target relevant shards
        if query_filter.jurisdictions:
            for jurisdiction in query_filter.jurisdictions:
                for shard_name, config in self.shard_configurations.items():
                    if any(primary_jurisdiction.lower() in jurisdiction.lower() 
                          for primary_jurisdiction in config.primary_jurisdictions):
                        target_shards.add(shard_name)
        
        # If specific document types requested, target relevant shards
        if query_filter.document_types:
            for doc_type in query_filter.document_types:
                for shard_name, config in self.shard_configurations.items():
                    if doc_type in config.document_types:
                        target_shards.add(shard_name)
        
        # If no specific criteria, query all shards
        if not target_shards:
            target_shards = set(self.shard_configurations.keys())
        
        # Sort by priority for optimal query execution
        sorted_shards = sorted(target_shards, 
                              key=lambda x: self.shard_configurations[x].priority_level)
        
        logger.info(f"üîç Query will target {len(sorted_shards)} shards: {sorted_shards}")
        return sorted_shards

class UltraScaleDatabaseService:
    """
    Ultra-Scale Database Service for 370M+ Legal Documents
    Implements distributed architecture with geographic sharding
    """
    
    def __init__(self, mongo_url: str):
        self.mongo_url = mongo_url
        self.client: Optional[AsyncIOMotorClient] = None
        self.databases: Dict[str, AsyncIOMotorDatabase] = {}
        self.collections: Dict[str, AsyncIOMotorCollection] = {}
        
        # Sharding and performance
        self.sharding_strategy = GeographicShardingStrategy()
        self.performance_metrics: List[QueryPerformanceMetrics] = []
        self.connection_pools: Dict[str, AsyncIOMotorClient] = {}
        
        # Caching and optimization
        self.query_cache: Dict[str, Tuple[Any, datetime]] = {}
        self.cache_ttl = timedelta(minutes=15)
        
        logger.info("üöÄ UltraScaleDatabaseService initialized for 370M+ documents")
    
    async def initialize_ultra_scale_architecture(self):
        """Initialize the complete ultra-scale database architecture"""
        logger.info("üèóÔ∏è Initializing ultra-scale database architecture...")
        
        try:
            # Step 1: Initialize database connections for all shards
            await self._initialize_shard_connections()
            
            # Step 2: Create ultra-scale indexes
            await self.create_ultra_scale_indexes()
            
            # Step 3: Initialize sharding configurations
            await self._configure_sharding()
            
            # Step 4: Initialize performance monitoring
            await self._initialize_performance_monitoring()
            
            logger.info("‚úÖ Ultra-scale database architecture initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error initializing ultra-scale architecture: {e}")
            raise
    
    async def _initialize_shard_connections(self):
        """Initialize optimized connections for each shard"""
        self.client = AsyncIOMotorClient(self.mongo_url)
        
        for shard_name, config in self.sharding_strategy.shard_configurations.items():
            # Create database for this shard
            db_name = f"legal_documents_{shard_name}"
            self.databases[shard_name] = self.client[db_name]
            
            # Create collection for documents in this shard
            collection_name = "documents"
            self.collections[shard_name] = self.databases[shard_name][collection_name]
            
            logger.info(f"üìä Initialized shard '{shard_name}' -> database: {db_name}")
    
    async def create_ultra_scale_indexes(self):
        """Create optimized indexes for 370M+ document queries - Step 3.1 Implementation"""
        logger.info("üîß Creating ultra-scale indexes for optimal performance...")
        
        # Step 3.1: Define comprehensive index strategy for massive scale
        index_definitions = [
            # Geographic/Jurisdictional Indexes (Primary routing)
            IndexModel([
                ("jurisdiction", ASCENDING),
                ("jurisdiction_level", ASCENDING), 
                ("document_type", ASCENDING),
                ("date_published", DESCENDING)
            ], name="jurisdiction_type_date_idx", background=True),
            
            # Legal Topic Indexes (Content-based queries)
            IndexModel([
                ("legal_topics", ASCENDING),
                ("precedential_value", DESCENDING),
                ("confidence_score", DESCENDING)
            ], name="topics_precedential_quality_idx", background=True),
            
            # Source & Quality Indexes (Administrative queries)
            IndexModel([
                ("source", ASCENDING),
                ("processing_status", ASCENDING),
                ("quality_score", DESCENDING)
            ], name="source_status_quality_idx", background=True),
            
            # Full-text Search Indexes (Text search optimization)
            IndexModel([
                ("title", TEXT),
                ("content", TEXT),
                ("searchable_text", TEXT)
            ], name="fulltext_search_idx", background=True),
            
            # Citation Network Indexes (Legal research)
            IndexModel([
                ("citations", ASCENDING)
            ], name="citations_idx", background=True),
            
            IndexModel([
                ("cited_by", ASCENDING)
            ], name="cited_by_idx", background=True),
            
            # Date-based Indexes (Temporal queries)
            IndexModel([
                ("date_published", DESCENDING),
                ("date_filed", DESCENDING)
            ], name="dates_idx", background=True),
            
            # Court & Authority Indexes (Institutional queries)
            IndexModel([
                ("court", ASCENDING),
                ("jurisdiction", ASCENDING),
                ("document_type", ASCENDING)
            ], name="court_jurisdiction_type_idx", background=True),
            
            # Document Status & Processing (Administrative)
            IndexModel([
                ("processing_status", ASCENDING),
                ("created_at", DESCENDING)
            ], name="processing_status_created_idx", background=True),
            
            # Parties & Entities (Legal relationship queries)
            IndexModel([
                ("parties", ASCENDING)
            ], name="parties_idx", background=True),
            
            # Document Relationships (Citation networks)
            IndexModel([
                ("related_documents", ASCENDING)
            ], name="related_documents_idx", background=True),
            
            # Compound Indexes for Complex Queries
            IndexModel([
                ("jurisdiction", ASCENDING),
                ("document_type", ASCENDING),
                ("legal_topics", ASCENDING),
                ("date_published", DESCENDING)
            ], name="complex_query_idx", background=True),
            
            # Performance Optimization Indexes
            IndexModel([
                ("confidence_score", DESCENDING),
                ("completeness_score", DESCENDING)
            ], name="quality_metrics_idx", background=True)
        ]
        
        # Apply indexes to all shards
        index_creation_tasks = []
        for shard_name, collection in self.collections.items():
            logger.info(f"üîß Creating {len(index_definitions)} indexes for shard: {shard_name}")
            
            # Create indexes with error handling
            task = self._create_indexes_for_shard(shard_name, collection, index_definitions)
            index_creation_tasks.append(task)
        
        # Execute index creation in parallel across all shards
        results = await asyncio.gather(*index_creation_tasks, return_exceptions=True)
        
        successful_shards = 0
        for i, result in enumerate(results):
            shard_name = list(self.collections.keys())[i]
            if isinstance(result, Exception):
                logger.error(f"‚ùå Failed to create indexes for shard {shard_name}: {result}")
            else:
                successful_shards += 1
                logger.info(f"‚úÖ Successfully created indexes for shard {shard_name}")
        
        logger.info(f"üéØ Index creation completed: {successful_shards}/{len(self.collections)} shards successful")
    
    async def _create_indexes_for_shard(self, shard_name: str, collection: AsyncIOMotorCollection, 
                                      index_definitions: List[IndexModel]):
        """Create indexes for a specific shard with error handling"""
        try:
            await collection.create_indexes(index_definitions)
            
            # Verify indexes were created
            index_info = await collection.index_information()
            created_count = len([name for name in index_info.keys() if not name.startswith('_')])
            
            logger.info(f"üìä Shard '{shard_name}': Created {created_count} indexes successfully")
            return created_count
            
        except Exception as e:
            logger.error(f"‚ùå Error creating indexes for shard {shard_name}: {e}")
            raise
    
    async def _configure_sharding(self):
        """Configure advanced sharding parameters"""
        logger.info("‚öôÔ∏è Configuring advanced sharding parameters...")
        
        try:
            for shard_name, config in self.sharding_strategy.shard_configurations.items():
                db = self.databases[shard_name]
                
                # Configure collection options for optimal performance
                collection_options = {
                    'storageEngine': {'wiredTiger': {
                        'configString': 'block_compressor=zstd'  # Better compression
                    }}
                }
                
                # Set read preferences for optimal performance
                collection = self.collections[shard_name]
                collection = collection.with_options(
                    read_preference=pymongo.read_preferences.ReadPreference.PRIMARY_PREFERRED,
                    write_concern=pymongo.write_concern.WriteConcern(w=config.write_concern)
                )
                
                self.collections[shard_name] = collection
                logger.info(f"‚öôÔ∏è Configured shard '{shard_name}' with optimized settings")
                
        except Exception as e:
            logger.error(f"‚ùå Error configuring sharding: {e}")
            raise
    
    async def _initialize_performance_monitoring(self):
        """Initialize performance monitoring for all shards"""
        logger.info("üìà Initializing performance monitoring...")
        
        # Clear existing metrics
        self.performance_metrics.clear()
        
        # Test connectivity to all shards
        connectivity_tasks = []
        for shard_name, collection in self.collections.items():
            task = self._test_shard_connectivity(shard_name, collection)
            connectivity_tasks.append(task)
        
        results = await asyncio.gather(*connectivity_tasks, return_exceptions=True)
        
        active_shards = 0
        for i, result in enumerate(results):
            shard_name = list(self.collections.keys())[i]
            if isinstance(result, Exception):
                logger.error(f"‚ùå Shard {shard_name} connectivity failed: {result}")
            else:
                active_shards += 1
                logger.info(f"‚úÖ Shard {shard_name} connectivity verified")
        
        logger.info(f"üìà Performance monitoring initialized: {active_shards}/{len(self.collections)} shards active")
    
    async def _test_shard_connectivity(self, shard_name: str, collection: AsyncIOMotorCollection) -> bool:
        """Test connectivity to a specific shard"""
        try:
            # Simple ping operation
            await collection.find_one({}, {"_id": 1})
            return True
        except Exception as e:
            logger.error(f"Connectivity test failed for shard {shard_name}: {e}")
            return False
    
    # ================================================================================================
    # DOCUMENT OPERATIONS - ULTRA-SCALE IMPLEMENTATIONS
    # ================================================================================================
    
    async def create_document(self, document_data: LegalDocumentCreate) -> LegalDocument:
        """Create a new legal document with automatic shard routing"""
        start_time = time.time()
        
        try:
            # Determine optimal shard for this document
            target_shard = self.sharding_strategy.determine_shard(document_data)
            collection = self.collections[target_shard]
            
            # Create document with enhanced metadata
            document = LegalDocument(**document_data.dict())
            document_dict = document.dict()
            
            # Add sharding metadata
            document_dict['_shard'] = target_shard
            document_dict['_shard_timestamp'] = datetime.utcnow()
            
            # Insert into target shard
            result = await collection.insert_one(document_dict)
            document.id = str(result.inserted_id) if not document.id else document.id
            
            # Record performance metrics
            execution_time = (time.time() - start_time) * 1000
            await self._record_query_metrics('create_document', target_shard, execution_time, 0, 1, True)
            
            logger.info(f"‚úÖ Created document {document.id} in shard '{target_shard}'")
            return document
            
        except Exception as e:
            logger.error(f"‚ùå Error creating document: {e}")
            raise
    
    async def create_documents_bulk(self, documents_data: List[LegalDocumentCreate]) -> List[str]:
        """Create multiple documents in bulk with intelligent shard distribution"""
        start_time = time.time()
        logger.info(f"üì¶ Starting bulk creation of {len(documents_data)} documents...")
        
        try:
            # Group documents by target shard for optimal bulk operations
            shard_groups = defaultdict(list)
            document_objects = []
            
            for doc_data in documents_data:
                target_shard = self.sharding_strategy.determine_shard(doc_data)
                document = LegalDocument(**doc_data.dict())
                document_dict = document.dict()
                document_dict['_shard'] = target_shard
                document_dict['_shard_timestamp'] = datetime.utcnow()
                
                shard_groups[target_shard].append(document_dict)
                document_objects.append(document)
            
            # Execute bulk inserts per shard in parallel
            insert_tasks = []
            for shard_name, documents in shard_groups.items():
                collection = self.collections[shard_name]
                task = self._bulk_insert_to_shard(shard_name, collection, documents)
                insert_tasks.append(task)
            
            # Wait for all shard operations to complete
            shard_results = await asyncio.gather(*insert_tasks, return_exceptions=True)
            
            # Collect all document IDs
            all_document_ids = []
            successful_inserts = 0
            
            for i, result in enumerate(shard_results):
                shard_name = list(shard_groups.keys())[i]
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Bulk insert failed for shard {shard_name}: {result}")
                else:
                    successful_inserts += result['inserted_count']
                    all_document_ids.extend(result['document_ids'])
            
            # Record overall performance metrics
            execution_time = (time.time() - start_time) * 1000
            await self._record_query_metrics('create_documents_bulk', 'multiple_shards', 
                                           execution_time, len(documents_data), successful_inserts, True)
            
            logger.info(f"‚úÖ Bulk creation completed: {successful_inserts}/{len(documents_data)} documents inserted across {len(shard_groups)} shards")
            
            return all_document_ids
            
        except Exception as e:
            logger.error(f"‚ùå Error in bulk document creation: {e}")
            raise
    
    async def _bulk_insert_to_shard(self, shard_name: str, collection: AsyncIOMotorCollection, 
                                  documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Execute bulk insert operation for a specific shard"""
        try:
            # Use ordered=False for better performance (parallel inserts)
            result = await collection.insert_many(documents, ordered=False)
            
            # Extract document IDs
            document_ids = [doc.get('id', str(oid)) for doc, oid in zip(documents, result.inserted_ids)]
            
            logger.info(f"üì¶ Shard '{shard_name}': Inserted {len(result.inserted_ids)} documents")
            
            return {
                'shard_name': shard_name,
                'inserted_count': len(result.inserted_ids),
                'document_ids': document_ids
            }
            
        except Exception as e:
            logger.error(f"‚ùå Bulk insert failed for shard {shard_name}: {e}")
            raise
    
    async def search_documents(self, filter_params: LegalDocumentFilter, 
                             page: int = 1, per_page: int = 50) -> LegalDocumentResponse:
        """Execute distributed search across relevant shards"""
        start_time = time.time()
        logger.info(f"üîç Starting distributed search across shards...")
        
        try:
            # Determine which shards to query based on filter criteria
            target_shards = self.sharding_strategy.get_query_shards(filter_params)
            
            # Check query cache first
            cache_key = self._generate_cache_key(filter_params, page, per_page)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                logger.info("‚ö° Returning cached search results")
                return cached_result
            
            # Build MongoDB query from filter parameters
            query = self._build_search_query(filter_params)
            
            # Execute parallel queries across target shards
            search_tasks = []
            for shard_name in target_shards:
                collection = self.collections[shard_name]
                task = self._search_shard(shard_name, collection, query, page, per_page)
                search_tasks.append(task)
            
            # Wait for all shard queries to complete
            shard_results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # Aggregate results from all shards
            aggregated_result = await self._aggregate_search_results(shard_results, target_shards, 
                                                                   page, per_page, filter_params)
            
            # Cache the result for future queries
            self._cache_result(cache_key, aggregated_result)
            
            # Record performance metrics
            execution_time = (time.time() - start_time) * 1000
            await self._record_query_metrics('search_documents', f"{len(target_shards)}_shards", 
                                           execution_time, aggregated_result.total_count, 
                                           len(aggregated_result.documents), True)
            
            logger.info(f"‚úÖ Distributed search completed: {aggregated_result.total_count} total documents found across {len(target_shards)} shards")
            
            return aggregated_result
            
        except Exception as e:
            logger.error(f"‚ùå Error in distributed search: {e}")
            raise
    
    async def _search_shard(self, shard_name: str, collection: AsyncIOMotorCollection, 
                          query: Dict[str, Any], page: int, per_page: int) -> Dict[str, Any]:
        """Execute search query on a specific shard"""
        try:
            # Count total documents matching query in this shard
            total_count = await collection.count_documents(query)
            
            if total_count == 0:
                return {
                    'shard_name': shard_name,
                    'documents': [],
                    'total_count': 0,
                    'execution_time_ms': 0
                }
            
            # Calculate skip for pagination (distribute across shards)
            skip = max(0, (page - 1) * per_page)
            
            # Execute query with pagination
            cursor = collection.find(query).skip(skip).limit(per_page)
            cursor = cursor.sort("date_published", DESCENDING)  # Default sort
            
            documents_data = await cursor.to_list(length=per_page)
            documents = [LegalDocument(**doc) for doc in documents_data]
            
            logger.debug(f"üîç Shard '{shard_name}': Found {len(documents)}/{total_count} documents")
            
            return {
                'shard_name': shard_name,
                'documents': documents,
                'total_count': total_count,
                'documents_returned': len(documents)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Search failed for shard {shard_name}: {e}")
            return {
                'shard_name': shard_name,
                'documents': [],
                'total_count': 0,
                'error': str(e)
            }
    
    def _build_search_query(self, filter_params: LegalDocumentFilter) -> Dict[str, Any]:
        """Build MongoDB query from filter parameters"""
        query = {}
        
        # Document types filter
        if filter_params.document_types:
            query["document_type"] = {"$in": filter_params.document_types}
        
        # Jurisdictions filter
        if filter_params.jurisdictions:
            query["jurisdiction"] = {"$in": filter_params.jurisdictions}
        
        # Jurisdiction levels filter
        if filter_params.jurisdiction_levels:
            query["jurisdiction_level"] = {"$in": filter_params.jurisdiction_levels}
        
        # Courts filter
        if filter_params.courts:
            query["court"] = {"$in": filter_params.courts}
        
        # Date range filter
        date_query = {}
        if filter_params.date_from:
            date_query["$gte"] = filter_params.date_from
        if filter_params.date_to:
            date_query["$lte"] = filter_params.date_to
        if date_query:
            query["date_published"] = date_query
        
        # Legal topics filter
        if filter_params.legal_topics:
            query["legal_topics"] = {"$in": filter_params.legal_topics}
        
        # Practice areas filter
        if filter_params.practice_areas:
            query["practice_areas"] = {"$in": filter_params.practice_areas}
        
        # Precedential values filter
        if filter_params.precedential_values:
            query["precedential_value"] = {"$in": filter_params.precedential_values}
        
        # Sources filter
        if filter_params.sources:
            query["source"] = {"$in": filter_params.sources}
        
        # Text search
        if filter_params.search_text:
            query["$text"] = {"$search": filter_params.search_text}
        
        # Confidence score filter
        if filter_params.min_confidence_score:
            query["confidence_score"] = {"$gte": filter_params.min_confidence_score}
        
        # Processing status filter
        if filter_params.processing_status:
            query["processing_status"] = {"$in": filter_params.processing_status}
        
        return query
    
    async def _aggregate_search_results(self, shard_results: List[Any], target_shards: List[str],
                                      page: int, per_page: int, 
                                      filter_params: LegalDocumentFilter) -> LegalDocumentResponse:
        """Aggregate search results from multiple shards"""
        all_documents = []
        total_count = 0
        successful_shards = 0
        
        # Collect results from all shards
        for i, result in enumerate(shard_results):
            shard_name = target_shards[i]
            
            if isinstance(result, Exception):
                logger.error(f"‚ùå Search failed for shard {shard_name}: {result}")
                continue
            
            if 'error' in result:
                logger.error(f"‚ùå Search error for shard {shard_name}: {result['error']}")
                continue
            
            all_documents.extend(result['documents'])
            total_count += result['total_count']
            successful_shards += 1
        
        # Sort combined results (re-sort since we combined from multiple shards)
        all_documents.sort(key=lambda x: x.date_published or datetime.min, reverse=True)
        
        # Apply pagination to combined results
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        paginated_documents = all_documents[start_idx:end_idx]
        
        # Calculate total pages based on combined count
        total_pages = (total_count + per_page - 1) // per_page
        
        return LegalDocumentResponse(
            documents=paginated_documents,
            total_count=total_count,
            page=page,
            per_page=per_page,
            total_pages=total_pages,
            filters_applied=filter_params,
            search_metadata={
                'shards_queried': len(target_shards),
                'successful_shards': successful_shards,
                'documents_from_shards': len(all_documents)
            }
        )
    
    # ================================================================================================
    # CACHING AND PERFORMANCE OPTIMIZATION
    # ================================================================================================
    
    def _generate_cache_key(self, filter_params: LegalDocumentFilter, page: int, per_page: int) -> str:
        """Generate cache key for query results"""
        filter_dict = filter_params.dict(exclude_none=True)
        cache_data = {
            'filters': filter_dict,
            'page': page,
            'per_page': per_page
        }
        cache_str = json.dumps(cache_data, sort_keys=True, default=str)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[LegalDocumentResponse]:
        """Retrieve cached query result if still valid"""
        if cache_key in self.query_cache:
            result, timestamp = self.query_cache[cache_key]
            if datetime.utcnow() - timestamp < self.cache_ttl:
                return result
            else:
                # Remove expired cache entry
                del self.query_cache[cache_key]
        return None
    
    def _cache_result(self, cache_key: str, result: LegalDocumentResponse):
        """Cache query result with timestamp"""
        self.query_cache[cache_key] = (result, datetime.utcnow())
        
        # Limit cache size (remove oldest entries)
        if len(self.query_cache) > 1000:
            oldest_key = min(self.query_cache.keys(), 
                           key=lambda k: self.query_cache[k][1])
            del self.query_cache[oldest_key]
    
    async def _record_query_metrics(self, query_type: str, shard_name: str, 
                                  execution_time_ms: float, documents_scanned: int, 
                                  documents_returned: int, index_used: bool):
        """Record query performance metrics"""
        metric = QueryPerformanceMetrics(
            query_type=query_type,
            shard_name=shard_name,
            execution_time_ms=execution_time_ms,
            documents_scanned=documents_scanned,
            documents_returned=documents_returned,
            index_used=index_used
        )
        
        self.performance_metrics.append(metric)
        
        # Keep only recent metrics (last 10,000 queries)
        if len(self.performance_metrics) > 10000:
            self.performance_metrics = self.performance_metrics[-8000:]  # Keep last 8,000
    
    # ================================================================================================
    # SYSTEM METRICS AND MONITORING
    # ================================================================================================
    
    async def get_ultra_scale_system_metrics(self) -> Dict[str, Any]:
        """Get comprehensive system metrics for ultra-scale deployment"""
        logger.info("üìä Generating ultra-scale system metrics...")
        
        try:
            metrics_tasks = []
            
            # Collect metrics from all shards in parallel
            for shard_name in self.collections.keys():
                task = self._get_shard_metrics(shard_name)
                metrics_tasks.append(task)
            
            shard_metrics_results = await asyncio.gather(*metrics_tasks, return_exceptions=True)
            
            # Aggregate shard metrics
            total_documents = 0
            total_size_mb = 0
            shard_details = {}
            active_shards = 0
            
            for i, result in enumerate(shard_metrics_results):
                shard_name = list(self.collections.keys())[i]
                
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Failed to get metrics for shard {shard_name}: {result}")
                    shard_details[shard_name] = {
                        'status': 'error',
                        'error': str(result)
                    }
                else:
                    active_shards += 1
                    total_documents += result['document_count']
                    total_size_mb += result['size_mb']
                    shard_details[shard_name] = result
            
            # Performance metrics analysis
            performance_summary = self._analyze_performance_metrics()
            
            return {
                'timestamp': datetime.utcnow(),
                'total_documents': total_documents,
                'total_size_mb': total_size_mb,
                'active_shards': active_shards,
                'total_shards': len(self.collections),
                'shard_details': shard_details,
                'performance_summary': performance_summary,
                'architecture': {
                    'sharding_strategy': 'geographic',
                    'total_indexes_per_shard': 13,
                    'cache_entries': len(self.query_cache),
                    'performance_metrics_recorded': len(self.performance_metrics)
                },
                'capacity_utilization': {
                    'estimated_total_capacity': 370_000_000,
                    'current_utilization': total_documents,
                    'utilization_percentage': (total_documents / 370_000_000) * 100
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error generating system metrics: {e}")
            raise
    
    async def _get_shard_metrics(self, shard_name: str) -> Dict[str, Any]:
        """Get detailed metrics for a specific shard"""
        try:
            collection = self.collections[shard_name]
            db = self.databases[shard_name]
            
            # Document count
            document_count = await collection.count_documents({})
            
            # Collection stats
            stats = await db.command("collStats", "documents")
            size_mb = stats.get('size', 0) / (1024 * 1024)
            
            # Index information
            index_info = await collection.index_information()
            index_count = len(index_info)
            
            # Recent activity (documents created in last 24 hours)
            yesterday = datetime.utcnow() - timedelta(days=1)
            recent_documents = await collection.count_documents({
                "created_at": {"$gte": yesterday}
            })
            
            # Processing status distribution
            processing_pipeline = [
                {"$group": {
                    "_id": "$processing_status",
                    "count": {"$sum": 1}
                }}
            ]
            processing_stats = {}
            async for result in collection.aggregate(processing_pipeline):
                processing_stats[result['_id']] = result['count']
            
            return {
                'shard_name': shard_name,
                'status': 'active',
                'document_count': document_count,
                'size_mb': round(size_mb, 2),
                'index_count': index_count,
                'recent_documents_24h': recent_documents,
                'processing_status_distribution': processing_stats,
                'configuration': self.sharding_strategy.shard_configurations[shard_name].__dict__
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error getting metrics for shard {shard_name}: {e}")
            raise
    
    def _analyze_performance_metrics(self) -> Dict[str, Any]:
        """Analyze recent performance metrics"""
        if not self.performance_metrics:
            return {'status': 'no_metrics_available'}
        
        recent_metrics = [m for m in self.performance_metrics 
                         if datetime.utcnow() - m.timestamp < timedelta(hours=1)]
        
        if not recent_metrics:
            return {'status': 'no_recent_metrics'}
        
        # Calculate performance statistics
        execution_times = [m.execution_time_ms for m in recent_metrics]
        
        return {
            'total_queries_last_hour': len(recent_metrics),
            'average_execution_time_ms': round(statistics.mean(execution_times), 2),
            'median_execution_time_ms': round(statistics.median(execution_times), 2),
            'max_execution_time_ms': max(execution_times),
            'min_execution_time_ms': min(execution_times),
            'queries_by_type': {
                query_type: len([m for m in recent_metrics if m.query_type == query_type])
                for query_type in set(m.query_type for m in recent_metrics)
            },
            'queries_by_shard': {
                shard: len([m for m in recent_metrics if m.shard_name == shard])
                for shard in set(m.shard_name for m in recent_metrics)
            }
        }
    
    async def get_all_shards(self) -> List[str]:
        """Get list of all available shards"""
        return list(self.collections.keys())
    
    async def close_connections(self):
        """Close all database connections"""
        if self.client:
            self.client.close()
            logger.info("üîí All database connections closed")

# ================================================================================================
# UTILITY FUNCTIONS FOR INTEGRATION
# ================================================================================================

async def create_ultra_scale_database_service(mongo_url: str) -> UltraScaleDatabaseService:
    """Factory function to create and initialize ultra-scale database service"""
    service = UltraScaleDatabaseService(mongo_url)
    await service.initialize_ultra_scale_architecture()
    return service

async def test_ultra_scale_performance(service: UltraScaleDatabaseService, 
                                     test_document_count: int = 1000) -> Dict[str, Any]:
    """Performance testing function for ultra-scale database"""
    logger.info(f"üß™ Starting performance test with {test_document_count} documents...")
    
    start_time = time.time()
    
    # Create test documents
    test_documents = []
    for i in range(test_document_count):
        doc = LegalDocumentCreate(
            title=f"Test Document {i}",
            content=f"This is test content for document {i}" * 10,  # Realistic size
            document_type=DocumentType.CASE_LAW,
            jurisdiction="United States" if i % 2 == 0 else "European Union",
            source="performance_test",
            source_url=f"https://test.com/doc/{i}",
            confidence_score=0.9
        )
        test_documents.append(doc)
    
    # Test bulk insert
    insert_start = time.time()
    document_ids = await service.create_documents_bulk(test_documents)
    insert_time = time.time() - insert_start
    
    # Test search performance
    search_start = time.time()
    filter_params = LegalDocumentFilter(jurisdictions=["United States"])
    results = await service.search_documents(filter_params, page=1, per_page=100)
    search_time = time.time() - search_start
    
    total_time = time.time() - start_time
    
    return {
        'test_summary': {
            'total_documents_created': len(document_ids),
            'total_test_time_seconds': round(total_time, 2),
            'insert_time_seconds': round(insert_time, 2),
            'search_time_seconds': round(search_time, 2),
            'insert_rate_docs_per_second': round(test_document_count / insert_time, 2),
            'search_results_found': results.total_count
        },
        'performance_metrics': {
            'average_insert_time_ms': round((insert_time * 1000) / test_document_count, 3),
            'search_response_time_ms': round(search_time * 1000, 2),
            'shards_utilized': len(await service.get_all_shards())
        }
    }