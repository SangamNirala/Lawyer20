#!/usr/bin/env python3
"""
Comprehensive Backend Test Suite for Step 6.1: Ultra-Scale Performance Optimization
Tests MongoDB caching, AI query optimization, and sub-2-second performance targets
"""

import asyncio
import logging
import sys
import os
import time
import json
import requests
from typing import Dict, List, Any
from datetime import datetime

# Add backend directory to path
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'backend'))

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Step61PerformanceTestSuite:
    """Comprehensive test suite for Step 6.1 Performance Optimization"""
    
    def __init__(self):
        self.test_results = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "test_details": [],
            "critical_issues": [],
            "minor_issues": []
        }
        
        # Get backend URL from environment
        self.backend_url = os.environ.get('REACT_APP_BACKEND_URL', 'https://legalextract.preview.emergentagent.com')
        self.api_base = f"{self.backend_url}/api"
        self.performance_api_base = f"{self.backend_url}/api/performance"
        
        # Test configuration
        self.performance_targets = {
            'max_query_time_ms': 2000,      # Sub-2-second requirement
            'min_cache_hit_rate': 85.0,     # 85%+ cache hit rate
            'max_memory_usage_mb': 4000,    # 4GB memory limit
        }
        
        # Sample test queries for performance validation
        self.test_queries = [
            {
                'name': 'Low Complexity Query',
                'query_filter': {
                    'query_text': 'constitutional law civil rights',
                    'jurisdictions': ['United States'],
                    'document_types': ['CASE_LAW']
                },
                'page': 1,
                'per_page': 50,
                'expected_max_time_ms': 500
            },
            {
                'name': 'Medium Complexity Query',
                'query_filter': {
                    'query_text': 'intellectual property patent trademark',
                    'jurisdictions': ['United States', 'European Union'],
                    'document_types': ['CASE_LAW', 'STATUTE']
                },
                'page': 1,
                'per_page': 100,
                'expected_max_time_ms': 1000
            },
            {
                'name': 'High Complexity Query',
                'query_filter': {
                    'query_text': 'employment discrimination civil rights constitutional',
                    'jurisdictions': ['United States Federal', 'California', 'New York'],
                    'document_types': ['CASE_LAW', 'STATUTE', 'REGULATION']
                },
                'page': 1,
                'per_page': 200,
                'expected_max_time_ms': 1500
            }
        ]
        
    def log_test_result(self, test_name: str, success: bool, details: str = "", critical: bool = False):
        """Log test result"""
        self.test_results["total_tests"] += 1
        
        if success:
            self.test_results["passed_tests"] += 1
            status = "‚úÖ PASS"
        else:
            self.test_results["failed_tests"] += 1
            status = "‚ùå FAIL"
            if critical:
                self.test_results["critical_issues"].append(f"{test_name}: {details}")
            else:
                self.test_results["minor_issues"].append(f"{test_name}: {details}")
        
        result = {
            "test_name": test_name,
            "status": status,
            "success": success,
            "details": details,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.test_results["test_details"].append(result)
        print(f"{status} {test_name}")
        if details:
            print(f"    {details}")
    
    async def test_performance_system_status(self):
        """Test 1: Performance system status endpoint"""
        print("\nüöÄ TESTING PERFORMANCE SYSTEM STATUS")
        print("=" * 60)
        
        try:
            response = requests.get(f"{self.performance_api_base}/system-status", timeout=15)
            
            if response.status_code == 200:
                status_data = response.json()
                system_status = status_data.get('system_status', {})
                
                # Validate system status fields
                required_fields = [
                    'performance_system_status', 'sub_2_second_target', 'cache_health',
                    'optimization_status', 'mongodb_cache_status', 'application_cache_status'
                ]
                
                missing_fields = [field for field in required_fields if field not in system_status]
                
                self.log_test_result(
                    "Performance System Status API",
                    len(missing_fields) == 0,
                    f"Status: {system_status.get('performance_system_status', 'Unknown')}, Missing fields: {missing_fields}",
                    critical=True
                )
                
                # Check performance statistics
                perf_stats = system_status.get('performance_statistics', {})
                if perf_stats:
                    self.log_test_result(
                        "Performance Statistics Available",
                        True,
                        f"Cache efficiency: {perf_stats.get('cache_efficiency', 'N/A')}, "
                        f"Memory usage: {perf_stats.get('memory_usage', 'N/A')}"
                    )
                
                # Check capability metrics
                capabilities = system_status.get('capability_metrics', {})
                if capabilities:
                    self.log_test_result(
                        "System Capability Metrics",
                        True,
                        f"Concurrent capacity: {capabilities.get('concurrent_user_capacity', 'N/A')}, "
                        f"Query throughput: {capabilities.get('query_throughput', 'N/A')}"
                    )
                
            else:
                self.log_test_result(
                    "Performance System Status API",
                    False,
                    f"API returned status {response.status_code}: {response.text}",
                    critical=True
                )
                
        except Exception as e:
            self.log_test_result(
                "Performance System Status API",
                False,
                f"System status test failed: {str(e)}",
                critical=True
            )
    
    async def test_optimized_query_execution(self):
        """Test 2: Optimized query execution with caching"""
        print("\n‚ö° TESTING OPTIMIZED QUERY EXECUTION")
        print("=" * 60)
        
        for query_config in self.test_queries:
            try:
                query_name = query_config['name']
                print(f"\n  Testing: {query_name}")
                
                # Execute query twice to test caching
                first_execution_time = None
                second_execution_time = None
                cache_hit_achieved = False
                
                for attempt in range(2):
                    start_time = time.time()
                    
                    response = requests.post(
                        f"{self.performance_api_base}/optimize-query",
                        json=query_config,
                        timeout=30
                    )
                    
                    execution_time = (time.time() - start_time) * 1000
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        api_execution_time = result_data.get('execution_time_ms', execution_time)
                        cache_layer = result_data.get('cache_layer', 'none')
                        
                        if attempt == 0:
                            first_execution_time = api_execution_time
                            print(f"    First execution: {api_execution_time:.2f}ms ({cache_layer})")
                        else:
                            second_execution_time = api_execution_time
                            cache_hit_achieved = cache_layer in ['L1', 'L2']
                            print(f"    Second execution: {api_execution_time:.2f}ms ({cache_layer})")
                        
                        # Test sub-2-second performance
                        meets_performance_target = api_execution_time < self.performance_targets['max_query_time_ms']
                        
                        self.log_test_result(
                            f"{query_name} - Performance Target",
                            meets_performance_target,
                            f"Execution time: {api_execution_time:.2f}ms (target: <{self.performance_targets['max_query_time_ms']}ms)",
                            critical=not meets_performance_target
                        )
                        
                    else:
                        self.log_test_result(
                            f"{query_name} - API Response",
                            False,
                            f"Query failed with status {response.status_code}: {response.text}",
                            critical=True
                        )
                        break
                
                # Test caching effectiveness
                if first_execution_time and second_execution_time:
                    cache_improvement = ((first_execution_time - second_execution_time) / first_execution_time) * 100
                    
                    self.log_test_result(
                        f"{query_name} - Cache Performance",
                        cache_hit_achieved or cache_improvement > 10,
                        f"Cache hit: {cache_hit_achieved}, Improvement: {cache_improvement:.1f}%"
                    )
                
            except Exception as e:
                self.log_test_result(
                    f"{query_name} - Query Execution",
                    False,
                    f"Query execution failed: {str(e)}",
                    critical=True
                )
    
    async def test_cache_metrics_and_management(self):
        """Test 3: Cache metrics and management operations"""
        print("\nüìä TESTING CACHE METRICS AND MANAGEMENT")
        print("=" * 60)
        
        try:
            # Test cache metrics endpoint
            response = requests.get(f"{self.performance_api_base}/cache-metrics", timeout=15)
            
            if response.status_code == 200:
                metrics_data = response.json()
                cache_metrics = metrics_data.get('cache_metrics', {})
                
                # Validate L1 cache metrics
                l1_metrics = cache_metrics.get('L1_application_cache', {})
                if l1_metrics:
                    self.log_test_result(
                        "L1 Application Cache Metrics",
                        True,
                        f"Found {len(l1_metrics)} L1 cache types with metrics"
                    )
                    
                    # Check for expected cache types
                    expected_cache_types = ['hot_queries', 'user_sessions', 'api_metadata']
                    found_cache_types = list(l1_metrics.keys())
                    
                    for cache_type in expected_cache_types:
                        if cache_type in found_cache_types:
                            cache_data = l1_metrics[cache_type]
                            hit_rate = cache_data.get('hit_rate', 0)
                            
                            self.log_test_result(
                                f"L1 Cache Type: {cache_type}",
                                hit_rate >= 0,
                                f"Hit rate: {hit_rate:.1f}%, Size: {cache_data.get('cache_size_mb', 0):.2f}MB"
                            )
                
                # Validate L2 cache metrics
                l2_metrics = cache_metrics.get('L2_mongodb_cache', {})
                if l2_metrics:
                    self.log_test_result(
                        "L2 MongoDB Cache Metrics",
                        True,
                        f"L2 cache collections: {len(l2_metrics.get('collections', {}))}"
                    )
                
                # Test system memory metrics
                system_memory = cache_metrics.get('system_memory', {})
                if system_memory:
                    memory_usage = system_memory.get('application_cache_mb', 0)
                    memory_percentage = system_memory.get('memory_usage_percentage', 0)
                    
                    self.log_test_result(
                        "System Memory Usage",
                        memory_usage < self.performance_targets['max_memory_usage_mb'],
                        f"Memory usage: {memory_usage:.1f}MB ({memory_percentage:.1f}%)"
                    )
                
            else:
                self.log_test_result(
                    "Cache Metrics API",
                    False,
                    f"Cache metrics API returned status {response.status_code}",
                    critical=True
                )
            
            # Test cache management operations
            await self._test_cache_management_operations()
            
        except Exception as e:
            self.log_test_result(
                "Cache Metrics and Management",
                False,
                f"Cache metrics test failed: {str(e)}",
                critical=True
            )
    
    async def _test_cache_management_operations(self):
        """Test cache management operations"""
        try:
            # Test cache clear operation
            clear_request = {
                "operation": "clear",
                "cache_type": "L1"
            }
            
            response = requests.post(
                f"{self.performance_api_base}/cache-management",
                json=clear_request,
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                self.log_test_result(
                    "Cache Clear Operation",
                    result.get('status') == 'completed',
                    f"Operation: {result.get('operation')}, Status: {result.get('status')}"
                )
            else:
                self.log_test_result(
                    "Cache Clear Operation",
                    False,
                    f"Cache clear failed with status {response.status_code}"
                )
            
            # Test cache warm operation
            warm_request = {
                "operation": "warm",
                "parameters": {"query_count": 5}
            }
            
            response = requests.post(
                f"{self.performance_api_base}/cache-management",
                json=warm_request,
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                self.log_test_result(
                    "Cache Warm Operation",
                    result.get('status') in ['completed', 'queued'],
                    f"Operation: {result.get('operation')}, Status: {result.get('status')}"
                )
            else:
                self.log_test_result(
                    "Cache Warm Operation",
                    False,
                    f"Cache warm failed with status {response.status_code}"
                )
                
        except Exception as e:
            self.log_test_result(
                "Cache Management Operations",
                False,
                f"Cache management test failed: {str(e)}"
            )
    
    async def test_performance_dashboard(self):
        """Test 4: Performance dashboard and analytics"""
        print("\nüìà TESTING PERFORMANCE DASHBOARD")
        print("=" * 60)
        
        try:
            response = requests.get(f"{self.performance_api_base}/dashboard", timeout=20)
            
            if response.status_code == 200:
                dashboard_data = response.json()
                
                # Validate dashboard structure
                required_sections = [
                    'system_performance', 'cache_performance', 'query_optimization',
                    'performance_targets', 'optimization_insights'
                ]
                
                missing_sections = [section for section in required_sections if section not in dashboard_data]
                
                self.log_test_result(
                    "Performance Dashboard Structure",
                    len(missing_sections) == 0,
                    f"Dashboard sections: {list(dashboard_data.keys())}, Missing: {missing_sections}",
                    critical=len(missing_sections) > 0
                )
                
                # Validate cache performance data
                cache_performance = dashboard_data.get('cache_performance', {})
                if cache_performance:
                    overall_hit_rate = cache_performance.get('overall_hit_rate', 0)
                    cache_efficiency = cache_performance.get('cache_efficiency_score', 0)
                    
                    self.log_test_result(
                        "Cache Performance Metrics",
                        overall_hit_rate >= 0 and cache_efficiency >= 0,
                        f"Hit rate: {overall_hit_rate:.1f}%, Efficiency: {cache_efficiency:.1f}%"
                    )
                    
                    # Check if cache hit rate meets target
                    meets_cache_target = overall_hit_rate >= self.performance_targets['min_cache_hit_rate']
                    self.log_test_result(
                        "Cache Hit Rate Target",
                        meets_cache_target,
                        f"Hit rate: {overall_hit_rate:.1f}% (target: >={self.performance_targets['min_cache_hit_rate']}%)",
                        critical=not meets_cache_target
                    )
                
                # Validate system performance data
                system_performance = dashboard_data.get('system_performance', {})
                if system_performance:
                    sub_2_second_target = system_performance.get('sub_2_second_target', False)
                    
                    self.log_test_result(
                        "Sub-2-Second Performance Target",
                        sub_2_second_target,
                        f"Sub-2-second target status: {sub_2_second_target}",
                        critical=not sub_2_second_target
                    )
                
            else:
                self.log_test_result(
                    "Performance Dashboard API",
                    False,
                    f"Dashboard API returned status {response.status_code}: {response.text}",
                    critical=True
                )
                
        except Exception as e:
            self.log_test_result(
                "Performance Dashboard",
                False,
                f"Dashboard test failed: {str(e)}",
                critical=True
            )
    
    async def test_optimization_analytics(self):
        """Test 5: Optimization analytics and insights"""
        print("\nüîç TESTING OPTIMIZATION ANALYTICS")
        print("=" * 60)
        
        try:
            response = requests.get(f"{self.performance_api_base}/optimization-analytics", timeout=15)
            
            if response.status_code == 200:
                analytics_data = response.json()
                analytics = analytics_data.get('analytics', {})
                
                # Validate analytics structure
                required_sections = [
                    'optimization_summary', 'performance_trends', 'top_optimizations', 'system_impact'
                ]
                
                missing_sections = [section for section in required_sections if section not in analytics]
                
                self.log_test_result(
                    "Optimization Analytics Structure",
                    len(missing_sections) == 0,
                    f"Analytics sections: {list(analytics.keys())}, Missing: {missing_sections}"
                )
                
                # Validate optimization summary
                opt_summary = analytics.get('optimization_summary', {})
                if opt_summary:
                    total_optimizations = opt_summary.get('total_optimizations', 0)
                    avg_improvement = opt_summary.get('average_improvement_percentage', 0)
                    
                    self.log_test_result(
                        "Optimization Summary Metrics",
                        total_optimizations >= 0 and avg_improvement >= 0,
                        f"Total optimizations: {total_optimizations}, Avg improvement: {avg_improvement:.1f}%"
                    )
                
                # Validate top optimizations
                top_optimizations = analytics.get('top_optimizations', [])
                if top_optimizations:
                    self.log_test_result(
                        "Top Optimizations Data",
                        len(top_optimizations) > 0,
                        f"Found {len(top_optimizations)} optimization types"
                    )
                    
                    for opt in top_optimizations:
                        opt_type = opt.get('optimization_type', 'Unknown')
                        success_rate = opt.get('success_rate', 0)
                        print(f"    üìä {opt_type}: {success_rate:.1f}% success rate")
                
            else:
                self.log_test_result(
                    "Optimization Analytics API",
                    False,
                    f"Analytics API returned status {response.status_code}",
                    critical=True
                )
                
        except Exception as e:
            self.log_test_result(
                "Optimization Analytics",
                False,
                f"Analytics test failed: {str(e)}",
                critical=True
            )
    
    async def test_performance_alerts(self):
        """Test 6: Performance alerts and monitoring"""
        print("\nüö® TESTING PERFORMANCE ALERTS")
        print("=" * 60)
        
        try:
            response = requests.get(f"{self.performance_api_base}/performance-alerts", timeout=15)
            
            if response.status_code == 200:
                alerts_data = response.json()
                alerts = alerts_data.get('alerts', [])
                
                self.log_test_result(
                    "Performance Alerts API",
                    True,
                    f"Retrieved {len(alerts)} performance alerts"
                )
                
                # Validate alert structure if alerts exist
                if alerts:
                    sample_alert = alerts[0]
                    required_fields = ['alert_id', 'alert_type', 'severity', 'message', 'timestamp']
                    missing_fields = [field for field in required_fields if field not in sample_alert]
                    
                    self.log_test_result(
                        "Alert Structure Validation",
                        len(missing_fields) == 0,
                        f"Alert fields: {list(sample_alert.keys())}, Missing: {missing_fields}"
                    )
                    
                    # Display alert information
                    for alert in alerts[:3]:  # Show first 3 alerts
                        alert_type = alert.get('alert_type', 'Unknown')
                        severity = alert.get('severity', 'Unknown')
                        message = alert.get('message', 'No message')
                        print(f"    üö® {severity.upper()}: {alert_type} - {message}")
                
            else:
                self.log_test_result(
                    "Performance Alerts API",
                    False,
                    f"Alerts API returned status {response.status_code}",
                    critical=True
                )
                
        except Exception as e:
            self.log_test_result(
                "Performance Alerts",
                False,
                f"Alerts test failed: {str(e)}",
                critical=True
            )
    
    async def test_concurrent_performance(self):
        """Test 7: Concurrent query performance"""
        print("\nüîÑ TESTING CONCURRENT PERFORMANCE")
        print("=" * 60)
        
        try:
            # Test concurrent queries
            concurrent_queries = 5  # Test with 5 concurrent queries
            query_config = self.test_queries[0]  # Use simple query for concurrency test
            
            async def execute_concurrent_query(query_id):
                """Execute a single query for concurrency testing"""
                try:
                    start_time = time.time()
                    response = requests.post(
                        f"{self.performance_api_base}/optimize-query",
                        json=query_config,
                        timeout=30
                    )
                    execution_time = (time.time() - start_time) * 1000
                    
                    return {
                        'query_id': query_id,
                        'success': response.status_code == 200,
                        'execution_time_ms': execution_time,
                        'response_data': response.json() if response.status_code == 200 else None
                    }
                except Exception as e:
                    return {
                        'query_id': query_id,
                        'success': False,
                        'execution_time_ms': 0,
                        'error': str(e)
                    }
            
            # Execute concurrent queries
            print(f"  Executing {concurrent_queries} concurrent queries...")
            start_time = time.time()
            
            # Use asyncio to simulate concurrent requests
            tasks = []
            for i in range(concurrent_queries):
                # Since requests is synchronous, we'll use threading simulation
                import threading
                import queue
                
            results_queue = queue.Queue()
            threads = []
            
            def thread_worker(query_id):
                result = asyncio.run(execute_concurrent_query(query_id))
                results_queue.put(result)
            
            # Start threads
            for i in range(concurrent_queries):
                thread = threading.Thread(target=thread_worker, args=(i,))
                threads.append(thread)
                thread.start()
            
            # Wait for all threads to complete
            for thread in threads:
                thread.join()
            
            # Collect results
            results = []
            while not results_queue.empty():
                results.append(results_queue.get())
            
            total_time = (time.time() - start_time) * 1000
            
            # Analyze results
            successful_queries = [r for r in results if r['success']]
            failed_queries = [r for r in results if not r['success']]
            
            success_rate = (len(successful_queries) / len(results)) * 100
            avg_execution_time = sum(r['execution_time_ms'] for r in successful_queries) / len(successful_queries) if successful_queries else 0
            
            self.log_test_result(
                "Concurrent Query Execution",
                success_rate >= 80,  # At least 80% success rate
                f"Success rate: {success_rate:.1f}% ({len(successful_queries)}/{len(results)}), "
                f"Avg time: {avg_execution_time:.2f}ms",
                critical=success_rate < 80
            )
            
            self.log_test_result(
                "Concurrent Performance Target",
                avg_execution_time < self.performance_targets['max_query_time_ms'],
                f"Average execution time: {avg_execution_time:.2f}ms (target: <{self.performance_targets['max_query_time_ms']}ms)"
            )
            
        except Exception as e:
            self.log_test_result(
                "Concurrent Performance Test",
                False,
                f"Concurrent performance test failed: {str(e)}",
                critical=True
            )
    
    async def test_integration_with_existing_systems(self):
        """Test 8: Integration with existing Step 4.1 API system"""
        print("\nüîó TESTING INTEGRATION WITH EXISTING SYSTEMS")
        print("=" * 60)
        
        try:
            # Test API info endpoint to check integration
            response = requests.get(f"{self.api_base}/api-info", timeout=15)
            
            if response.status_code == 200:
                api_info = response.json()
                features = api_info.get('features', {})
                
                # Check if performance optimization is available
                performance_available = features.get('performance_optimization', False)
                
                self.log_test_result(
                    "Performance Optimization Integration",
                    performance_available,
                    f"Performance optimization available: {performance_available}",
                    critical=not performance_available
                )
                
                # Check performance endpoints in API info
                perf_endpoints = api_info.get('endpoints', {}).get('performance_optimization_endpoints', {})
                expected_endpoints = [
                    'optimize_query', 'performance_dashboard', 'cache_metrics', 
                    'system_status', 'cache_management'
                ]
                
                available_endpoints = [ep for ep in expected_endpoints if perf_endpoints.get(ep) != "not_available"]
                
                self.log_test_result(
                    "Performance API Endpoints Integration",
                    len(available_endpoints) == len(expected_endpoints),
                    f"Available endpoints: {len(available_endpoints)}/{len(expected_endpoints)}"
                )
                
            else:
                self.log_test_result(
                    "API Integration Check",
                    False,
                    f"API info endpoint returned status {response.status_code}",
                    critical=True
                )
            
            # Test compatibility with Step 4.1 ultra-search
            if features.get('ultra_scale_api', False):
                self.log_test_result(
                    "Step 4.1 API Compatibility",
                    True,
                    "Step 4.1 ultra-scale API is available and compatible"
                )
            else:
                self.log_test_result(
                    "Step 4.1 API Compatibility",
                    False,
                    "Step 4.1 ultra-scale API not available",
                    critical=False  # Not critical for Step 6.1 testing
                )
                
        except Exception as e:
            self.log_test_result(
                "Integration with Existing Systems",
                False,
                f"Integration test failed: {str(e)}",
                critical=True
            )
    
    def print_test_summary(self):
        """Print comprehensive test summary"""
        print("\n" + "=" * 80)
        print("STEP 6.1 PERFORMANCE OPTIMIZATION TEST SUMMARY")
        print("=" * 80)
        
        total_tests = self.test_results["total_tests"]
        passed_tests = self.test_results["passed_tests"]
        failed_tests = self.test_results["failed_tests"]
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"üìä OVERALL RESULTS:")
        print(f"   Total Tests: {total_tests}")
        print(f"   Passed: {passed_tests} ‚úÖ")
        print(f"   Failed: {failed_tests} ‚ùå")
        print(f"   Success Rate: {success_rate:.1f}%")
        
        if self.test_results["critical_issues"]:
            print(f"\nüö® CRITICAL ISSUES ({len(self.test_results['critical_issues'])}):")
            for issue in self.test_results["critical_issues"]:
                print(f"   ‚ùå {issue}")
        
        if self.test_results["minor_issues"]:
            print(f"\n‚ö†Ô∏è MINOR ISSUES ({len(self.test_results['minor_issues'])}):")
            for issue in self.test_results["minor_issues"]:
                print(f"   ‚ö†Ô∏è {issue}")
        
        print(f"\nüéØ PERFORMANCE TARGETS VALIDATION:")
        print(f"   Sub-2-second queries: Target <{self.performance_targets['max_query_time_ms']}ms")
        print(f"   Cache hit rate: Target >={self.performance_targets['min_cache_hit_rate']}%")
        print(f"   Memory usage: Target <{self.performance_targets['max_memory_usage_mb']}MB")
        
        return {
            "success_rate": success_rate,
            "critical_issues": len(self.test_results["critical_issues"]),
            "minor_issues": len(self.test_results["minor_issues"]),
            "total_tests": total_tests
        }

async def main():
    """Main test execution function"""
    print("üöÄ STARTING STEP 6.1 ULTRA-SCALE PERFORMANCE OPTIMIZATION TESTS")
    print("=" * 80)
    
    test_suite = Step61PerformanceTestSuite()
    
    # Execute all tests
    await test_suite.test_performance_system_status()
    await test_suite.test_optimized_query_execution()
    await test_suite.test_cache_metrics_and_management()
    await test_suite.test_performance_dashboard()
    await test_suite.test_optimization_analytics()
    await test_suite.test_performance_alerts()
    await test_suite.test_concurrent_performance()
    await test_suite.test_integration_with_existing_systems()
    
    # Print final summary
    summary = test_suite.print_test_summary()
    
    return summary

if __name__ == "__main__":
    # Run the test suite
    summary = asyncio.run(main())
    
    # Exit with appropriate code
    if summary["critical_issues"] > 0:
        print(f"\n‚ùå TESTS FAILED: {summary['critical_issues']} critical issues found")
        sys.exit(1)
    else:
        print(f"\n‚úÖ TESTS PASSED: {summary['success_rate']:.1f}% success rate")
        sys.exit(0)