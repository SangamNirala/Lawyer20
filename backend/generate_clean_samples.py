#!/usr/bin/env python3
"""
ğŸ§¹ GENERATE CLEAN SAMPLE DOCUMENTS
=================================
Generate new sample documents with enhanced extraction
"""

import asyncio
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
import uuid

# Add current directory to path  
sys.path.append(str(Path(__file__).parent))

from browser_setup import DocumentExtractor
from ultra_comprehensive_global_sources import get_sources_by_tier

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def generate_clean_sample_documents():
    """Generate clean sample documents to replace the old ones"""
    logger.info("ğŸ§¹ GENERATING CLEAN SAMPLE DOCUMENTS")
    logger.info("=" * 50)
    
    # Create clean samples directory
    clean_dir = Path("/app/backend/clean_extracted_documents")
    clean_dir.mkdir(exist_ok=True)
    
    # Select diverse sources for demonstration
    sample_sources = []
    
    # Government sources (Tier 1)
    tier_1 = get_sources_by_tier(1)
    sample_sources.extend(list(tier_1.items())[:3])
    
    # International sources (Tier 2) 
    tier_2 = get_sources_by_tier(2)
    sample_sources.extend(list(tier_2.items())[:2])
    
    # Academic sources (Tier 3)
    tier_3 = get_sources_by_tier(3)
    sample_sources.extend(list(tier_3.items())[:1])
    
    clean_documents = []
    
    async with DocumentExtractor() as extractor:
        for i, (source_id, source_config) in enumerate(sample_sources, 1):
            logger.info(f"ğŸ”„ [{i}/{len(sample_sources)}] Processing: {source_config.name}")
            
            try:
                result = await extractor.extract_from_web(source_config)
                
                if result['status'] == 'success' and result['documents']:
                    doc = result['documents'][0]
                    
                    # Enhance document with additional metadata
                    enhanced_doc = {
                        "id": str(uuid.uuid4()),
                        "title": doc['title'],
                        "content": doc['content'],
                        "url": doc['url'],
                        "source": doc['source'],
                        "document_type": doc.get('document_type', 'administrative'),
                        "jurisdiction": doc.get('jurisdiction', 'Unknown'),
                        "extraction_metadata": {
                            "extracted_at": datetime.utcnow().isoformat(),
                            "extraction_method": doc.get('extraction_method', 'enhanced_web'),
                            "quality_score": doc.get('quality_score', 0.0),
                            "content_length": doc.get('content_length', 0),
                            "source_tier": get_source_tier(source_id),
                            "extraction_engine": "enhanced_intelligent_v1.0"
                        },
                        "metadata": doc.get('metadata', {}),
                        "clean_extraction": {
                            "html_removed": True,
                            "javascript_removed": True, 
                            "navigation_filtered": True,
                            "complete_content": True,
                            "human_readable": True
                        }
                    }
                    
                    # Save individual clean document
                    doc_filename = f"clean_{source_id}_doc_0.json"
                    doc_path = clean_dir / doc_filename
                    
                    with open(doc_path, 'w', encoding='utf-8') as f:
                        json.dump(enhanced_doc, f, indent=2, ensure_ascii=False)
                    
                    clean_documents.append(enhanced_doc)
                    
                    logger.info(f"   âœ… SUCCESS: {doc['content_length']:,} chars | Quality: {doc.get('quality_score', 0.0):.2f}")
                    logger.info(f"   ğŸ“„ Saved: {doc_filename}")
                    
                else:
                    error = result.get('error', 'Unknown error')
                    logger.warning(f"   âŒ FAILED: {error}")
                    
            except Exception as e:
                logger.error(f"   âŒ ERROR: {e}")
    
    # Generate summary report
    await generate_sample_summary(clean_documents, clean_dir)

def get_source_tier(source_id: str) -> int:
    """Get tier number for source"""
    for tier in range(1, 8):
        tier_sources = get_sources_by_tier(tier)
        if source_id in tier_sources:
            return tier
    return 0

async def generate_sample_summary(documents: list, output_dir: Path):
    """Generate summary of clean sample documents"""
    logger.info("")
    logger.info("ğŸ“Š CLEAN SAMPLE DOCUMENTS SUMMARY")
    logger.info("=" * 45)
    
    if not documents:
        logger.warning("No clean documents were generated!")
        return
    
    total_content = sum(doc['extraction_metadata']['content_length'] for doc in documents)
    avg_quality = sum(doc['extraction_metadata']['quality_score'] for doc in documents) / len(documents)
    
    logger.info(f"ğŸ“ˆ GENERATION RESULTS:")
    logger.info(f"   ğŸ“„ Documents Created: {len(documents)}")
    logger.info(f"   ğŸ“Š Total Content: {total_content:,} characters")
    logger.info(f"   ğŸ¯ Average Quality: {avg_quality:.2f}")
    logger.info("")
    
    logger.info(f"ğŸ“‹ DOCUMENT DETAILS:")
    for doc in documents:
        logger.info(f"   ğŸ”¸ {doc['source']}")
        logger.info(f"      ğŸ“„ {doc['extraction_metadata']['content_length']:,} chars")
        logger.info(f"      ğŸ¯ {doc['extraction_metadata']['quality_score']:.2f} quality")
        logger.info(f"      ğŸ“ \"{doc['title'][:60]}...\"")
    
    logger.info("")
    logger.info(f"ğŸŒŸ KEY FEATURES OF CLEAN DOCUMENTS:")
    logger.info(f"   âœ… 100% HTML-free content")
    logger.info(f"   âœ… No JavaScript or CSS")
    logger.info(f"   âœ… No navigation or boilerplate")
    logger.info(f"   âœ… Complete document content")
    logger.info(f"   âœ… Proper titles and metadata")
    logger.info(f"   âœ… Quality assessment scores")
    logger.info(f"   âœ… Perfect for NLP and chatbots")
    
    # Create combined summary file
    summary = {
        "generation_timestamp": datetime.utcnow().isoformat(),
        "total_documents": len(documents),
        "total_content_characters": total_content,
        "average_quality_score": avg_quality,
        "extraction_engine": "enhanced_intelligent_v1.0",
        "improvements": [
            "HTML tags completely removed",
            "JavaScript and CSS filtered out",
            "Navigation and boilerplate content eliminated",
            "Complete document content preserved",
            "Proper title extraction",
            "Quality assessment implemented",
            "Human-readable text format",
            "Optimized for NLP processing"
        ],
        "documents": documents
    }
    
    summary_file = output_dir / "clean_extraction_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ“„ Summary saved: {summary_file}")
    logger.info(f"ğŸ“‚ Clean documents directory: {output_dir}")
    logger.info("\nğŸš€ ENHANCED EXTRACTION SAMPLES READY!")

async def main():
    """Main execution"""
    await generate_clean_sample_documents()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Generation interrupted by user")
    except Exception as e:
        logger.error(f"âŒ Generation failed: {e}")
        import traceback
        traceback.print_exc()